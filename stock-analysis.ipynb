{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style= \"color:#9370DB;\"> Stock Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Libraries \n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# New liabraries. \n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# üìä Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as g\n",
    "\n",
    "# ü§ñ Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Stock Analysis Dataset:\n",
    "\n",
    "\n",
    "**First impressions:**\n",
    "    \n",
    "_____________\n",
    "\n",
    "The **S&P 500** is a stock market index tracking the performance of the largest 500 publicly traded companies listed on U.S. stock exchanges.\n",
    "\n",
    "Investors have long used the S&P 500 as a benchmark for their investments as it tends to signal overall market health. \n",
    "The index is a popular choice for long-term inverstors who wish to watch growth over the coming deacades. \n",
    "\n",
    "The dataset contains: \n",
    "- S&P 500 **Index**: Contains the daily price of the index, representing the overall performance of the 500 companies in the S&P 500.\n",
    "- S&P 500 **Stocks**: Includes the daily stock prices for each company within the index, providing insights into individual stock movements. \n",
    "- S&P 500 **Companies**: Provides detailed information about each company, including metrics such as Name, Sector, Marketcap, Ebitda, Weight.\n",
    "\n",
    "The data types are even: (13 int or float / 13 objects).\n",
    "\n",
    "Our **project goal** is to identify the performance of various sectors in the S&P 500. After reading the [documentation](https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks) we will proceed with the following **strategy**:\n",
    "\n",
    "1. The **target** of our dataset will be `currentprice`, which is the actual price of the stock right now.\n",
    "2. Through **Exploratory Data Analysis** we will identify the features that contribute to this prediction.\n",
    "\n",
    "\n",
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 01 | Data Extraction </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sp500_stocks.csv')\n",
    "df = pd.read_csv('sp500_companies.csv')\n",
    "sp = pd.read_csv('sp500_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning columns with snake_case \n",
    "data.columns = [col.lower().replace(\" \", \"_\")for col in data.columns] \n",
    "df.columns = [col.lower().replace(\" \", \"_\")for col in df.columns] \n",
    "sp.columns = [col.lower().replace(\" \", \"_\")for col in sp.columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\">1.1 | Exploring the Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description: \n",
    "\n",
    "A brief analysis of each column. \n",
    "- `Date`: The specific date for which the stock date is recorded. \n",
    "- `Symbol`: A unique \"ticker\" code that identifies the company on the stock exchange. \n",
    "- `Adj_close`: The closing price of the stock after adjustments for dividends, splits, or other corporate actions. \n",
    "- `Close`: The unadjusted closing price of the stock on a given date.  \n",
    "- `High`: The highest price at which the stock traded during the day.  \n",
    "- `Low`: The lowest price at which the stock traded during the day. \n",
    "- `Open`: The price at which the stock started trading at the beginning of the day.\n",
    "- `Volume`: The total number of shares traded during the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description: \n",
    "\n",
    "A brief analysis of each column: \n",
    "- `Exchange`: A marketplace where stocks, bonds or other comodities are traded. (Example: NYSE, NASDAQ).\n",
    "- `Symbol`: A unique \"ticker\" code that identifies the company on the stock exchange. \n",
    "- `Shortname`: The abbreviated name of the company. \n",
    "- `Longname`: The full name of the company. \n",
    "- `Sector`: The broader industry classification that the company belongs to, such as Technology, Healthcare, etc. \n",
    "- `Industry`: A more specific classification of the company's operations (e.g., Software, Pharmaceuticals).\n",
    "- `Currentprice`: The most recent price at which the company's stock was sold or bought. \n",
    "- `Marketprice`: The total market value of the company's outstanding shares, calculated as: Current Price X Outstanding Shares. \n",
    "- `Ebitda`: (Earnings Before Interest Taxes Depreciation and Amortization ) Measures how profitable a company is before paying interest, taxes, and taking depreciation and amortization. \n",
    "- `Revenuegrowth`: The percentage increase or decrease in sales between periods, calculated as: \n",
    "- `City`: The city where the company's headquarters is located. \n",
    "- `State`: The state where the company's headquarters is located.\n",
    "- `Country`: The country of the company's origin. \n",
    "- `Fulltimeemployees`: The total number of employes of the company's business activities. \n",
    "- `Longbusinesssummary`: A breif description and overview of the company's business activities. \n",
    "- `Weight`: Represents the weight of the company's market cap relative to the total market cap, used in index calculations in the S&P 500. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description: \n",
    "\n",
    "A brief analysis of each colunn: \n",
    "\n",
    "- `Date`: The specific date for which the S&P 500 date is recorded. \n",
    "- `s&p500`: The closing price of the S&P 500 on a given date.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\">1.2 | Copies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "df2 = df.copy()\n",
    "sp2 = sp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 02 | ‚öíÔ∏è Data Cleaning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.1 | Dealing with Data types</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.2 | Dealing with NaN values</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete NaN. TELL WHY I'M dropping \n",
    "data2.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this as a reference conunting the same dproing them that without droping them. \n",
    "data2.symbol.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.3 | Dealing with Duplicates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.5 | Dealing with outliers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_slayer(data): \n",
    "    \"\"\"\n",
    "    Automatically removes outliers based on Q1, Q3\n",
    "    \"\"\"\n",
    "    for column in data.select_dtypes(include=[np.number]):\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = outlier_slayer(df, \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.6 | Moving target to the right </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 2.7 | Other Steps </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Columns \n",
    "data2.drop(columns=['high', 'low', 'open','close'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to datetime. \n",
    "data2['date'] = pd.to_datetime(data2['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['year'] = data2['date'].dt.year\n",
    "data2['month'] = data2['date'].dt.month\n",
    "data2['day'] = data2['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year', 'month', 'day', 'symbol', 'adj_close', 'volume']\n",
    "data2 = data2[cols]\n",
    "data2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where year is between 2010 and 2013 because SP500 for comparison we do have 2014. \n",
    "data2.drop(data2[(data2['year'] >= 2010) & (data2['year'] <= 2014)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat helped. \n",
    "annual_returns = data2.groupby(['symbol', 'year']).apply(lambda group: (group['adj_close'].iloc[-1] / group['adj_close'].iloc[0]) - 1).reset_index(name='annual_return').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = annual_returns.pivot(index='symbol', columns='year', values='annual_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = pivoted_df.rename(columns={2015: 'ar_2015',2016:'ar_2016',2017: 'ar_2017', \n",
    "                                          2018:'ar_2018', 2019: 'ar_2019',2020: 'ar_2020', 2021: 'ar_2021', 2022:'ar_2022', 2023:'ar_2023',2024: 'ar_2024'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitive = pd.merge (df, pivoted_df, on='symbol')\n",
    "definitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Columns: Exchange, longname, longbusinesssumary. \n",
    "cols = ['symbol', 'shortname','sector','industry',\n",
    "        'marketcap','ebitda', 'revenuegrowth', 'city', 'state', 'country',   \n",
    "        'fulltimeemployees', 'weight', 'ar_2015', 'ar_2016', 'ar_2017', 'ar_2018', 'ar_2019',\n",
    "        'ar_2020', 'ar_2021', 'ar_2022', 'ar_2023', 'ar_2024', 'currentprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitive = definitive[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sector.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 03 | EDA (Exploratory Data Analysis) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> Optional | Selecting Numerical </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = definitive.select_dtypes(exclude='number')\n",
    "cat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = definitive.select_dtypes(include='number')\n",
    "num.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\">3.1 | Descriptive Statistics </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table = cat.sector.value_counts()\n",
    "proportion_table = cat.sector.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_proportion = pd.concat([frequency_table,proportion_table], axis = 1)\n",
    "frequency_proportion.columns = ['absolute_frequency', 'relative_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_absolute = frequency_table.sum()\n",
    "total_relative = proportion_table.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_row = pd.DataFrame({\n",
    "    'absolute_frequency': [total_absolute],\n",
    "    'relative_frequency': [total_relative]\n",
    "}, index=['Total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([frequency_proportion, total_row])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x='sector',\n",
    "    y='absolute_frequency',\n",
    "    data=frequency_proportion,\n",
    "    palette='viridis',\n",
    ")\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency table gives the count of each sector, while the proportion table provides the percentage representation of each category in the dataset. This helps to quickly identify dominant and minority categories. Such as:\n",
    "- Technology: 16% \n",
    "- Industrials: 14% \n",
    "- Financial Services: 13% \n",
    "- Healthcare: 12% \n",
    "- Consumer Cyclical: 10% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = definitive[definitive['sector'] == 'Technology']\n",
    "indus = definitive[definitive['sector'] == 'Industrials']\n",
    "fin = definitive[definitive['sector'] == 'Financial Services']\n",
    "health = definitive[definitive['sector'] == 'Healthcare']\n",
    "consumer = definitive[definitive['sector'] == 'Consumer Cyclical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 3.2 | Checking Distributions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#9370DB'\n",
    "\n",
    "nrows, ncols = 5, 4 \n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 16))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i >= len(num.columns):\n",
    "        ax.set_visible(False)  # hide unesed plots\n",
    "        continue\n",
    "    ax.hist(num.iloc[:, i], bins=30, color=color, edgecolor='black')\n",
    "    ax.set_title(num.columns[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 3.3 | Checking our target distribution</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without the filter 650.000 and taking out the outliers. \n",
    "sns.histplot(definitive[\"currentprice\"], color=color, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = sns.displot(data=definitive, x='currentprice', kde=True, height=8, aspect=1.6, bins=100, binrange=(0, 2100), color='mediumpurple')\n",
    "d2.set(xlabel='Current Price')\n",
    "plt.xlim(0, 2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson \n",
    "num.corrwith(definitive['currentprice']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman\n",
    "num.corrwith(df['currentprice'], method='spearman').sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\">3.4 | Checking Outliers </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#9370DB'\n",
    "\n",
    "# grid size\n",
    "nrows, ncols = 5, 4 \n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 16))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i >= len(num.columns):\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.boxplot(num.iloc[:, i].dropna(), vert=False, patch_artist=True, \n",
    "               boxprops=dict(facecolor=color, color='black'), \n",
    "               medianprops=dict(color='yellow'), whiskerprops=dict(color='black'), \n",
    "               capprops=dict(color='black'), flierprops=dict(marker='o', color='red', markersize=5))\n",
    "    ax.set_title(num.columns[i], fontsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\">3.5 | Looking for Correlations </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_corr = num.corr()\n",
    "num_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix-Heatmap Plot\n",
    "mask = np.zeros_like(num_corr)\n",
    "mask[np.triu_indices_from(mask)] = True \n",
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "ax = sns.heatmap(num_corr, mask=mask, annot=True, annot_kws={\"size\": 12}, linewidths=.5, cmap=\"BuPu\", fmt=\".2f\", ax=ax) # round to 2 decimal places\n",
    "ax.set_title(\"Correlation Heatmap\", fontsize=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scatter plots for each numerical column against 'currentprice' to visualize their relationships\n",
    "for col in num.columns:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.title('Scatter plot of price vs ' + col)\n",
    "    sns.scatterplot(data=definitive, x=col, y='currentprice')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **one-way ANOVA** to determine if there is a statistically significant difference in **stock price** based on **sector**.\n",
    "\n",
    "#### Define Hypotheses\n",
    "- **Null Hypothesis (H‚ÇÄ)**: There is no difference in mean stock prices between sectors such as at **Technolgies**, **Industrials**, and **Finance** companies.\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**: At least one group mean is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract salaries for Data Scientists by company size\n",
    "df_small = df[(df[\"job_title\"] == \"Data Scientist\") & (df[\"company_size\"] == \"Small\")][\"salary_in_usd\"]\n",
    "df_medium = df[(df[\"job_title\"] == \"Data Scientist\") & (df[\"company_size\"] == \"Medium\")][\"salary_in_usd\"]\n",
    "df_large = df[(df[\"job_title\"] == \"Data Scientist\") & (df[\"company_size\"] == \"Large\")][\"salary_in_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Way ANOVA\n",
    "f_stat, p_value = st.f_oneway(df_small, df_medium, df_large)\n",
    "print(f\"F-Statistic: {f_stat:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "print()\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Decision-Making\n",
    "if p_value > alpha:\n",
    "    print(\"Fail to Reject the Null Hypothesis: Company size has no significant impact on data scientist salaries.\")\n",
    "else:\n",
    "    print(\"Reject the Null Hypothesis: There is a significant difference in salaries based on company size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 04 | Data Processing </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 4.1 | X-Y Split</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 4.2 | Selecting the Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.1 | Selecting Model: Linear Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.2 | Selecting Model: Ridge Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.3 | Selecting Model: Lasso Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.4 | Selecting Model: Decision Tree Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.5 | Selecting Model: KNN Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #00BFFF;\"> 4.2.6 | Selecting Model: XGBoost Regression </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 4.3 | Final Comparision</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 05 | Improving Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 5.1 | Normalization with MinMaxScaler</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 5.2 | Standardization with StandardScaler</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 5.3 | Normzalization with Long Transform</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #4169E1;\"> 5.4 | Feature Engineering </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #9370DB;\"> 06 | Reporting </h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
